\chapter{Conclusions and recommendations}
\label{chapter:Conclusions}

An overall aim and five objectives for this thesis were outlined in Chapter \ref{chapter:Introduction}. Findings, with respect to these, are summarised in this chapter. Whilst conducting the research, a number of recommendations arose, both for future professional practice, and for further exploration and research.

\section{Leveraging advances in computing power for expedited flood simulation}

A comprehensive description of methods for applying a finite-volume solution to the shallow water equations, as a series of stencil operations suitable for heterogeneous devices, was presented in Chapter \ref{chapter:NumericalMethods}. This is the basis on which the new simulation software is founded, which while not the first to provide these capabilities, represents one of the earliest open-source codebases capable of operating on CPU, GPU and hybrid-style devices. Whilst many advocates of heterogeneous computing reported their methods improved performance by several orders of magnitude, the approximate 6.7$\times$ improvement was consistent with the vendor-quoted peak performance figures. A number of methods for mapping and caching the data used in the simulation was presented, which in many cases made little overall difference to the performance achievable, strengthening the case as a generalised software.

Application of the new methods to real-world historic flood events such as in Carlisle, described in Chapter \ref{chapter:Fluvial}, demonstrates heterogeneous computing makes the simulation of relatively long events (approximately three days), at detailed grid resolutions, a feasible option. However, to achieve more than an order of magnitude performance improvement, it is necessary to divide processing across multiple discrete devices. Two methods for domain decomposition were presented in Chapter \ref{chapter:Decomposition}, synchronising the timesteps between each domain, and forecasting a synchronisation point. These methods both allowed performance improvements to continue with increasing numbers of processing devices. However, the complexity associated with decoupling the timesteps between two or more domains creates new problems; no simulation is presented herein which benefited more from this method of decomposition, however it is possible in some circumstances. The author wishes to further the method to variable grid resolutions in each domain, and more complex domain shapes and decomposition algorithms, through which it is believed the decoupled timestep method may prove beneficial in catchment-scale simulation.

Synchronisation of timesteps between all domains makes for a simpler simulation, and is the method employed by other studies using decomposition across heterogeneous devices. This proves beneficial, but only for large simulations involving $\gtrapprox10^8$ cells.

For event-based simulations, if `Moore's Law' continues as expected, then the approaches presented herein will soon reduce simulation times further. However, failure to adopt methods capable of capitalising on multithreading or single-instruction multiple-data architectures will likely limit software performance, as clock speeds will likely increase very little. During the last five years as this thesis developed, commercial simulation software has begun to adapt on this basis. This is the only way in which the increasing volume and ubiquity of data can be harnessed, and failure to leverage this will create inferior simulation results, with far-reaching implications for the population and industry.

Simulations must exceed the cell counts and resolutions achievable in a timely manner on CPU devices, before GPU processing or domain decomposition becomes worthwhile. Extremely small domain simulations may even be slower using GPU devices, as the latency associated with large data transfers over the computer's host bus will remain a bottleneck. Nonetheless, consuming increasingly abundant and ubiquitous data will necessitate these techniques.

\section{Ability to produce accurate results comparable to existing software}

It is essential that any new methods for simulation are thoroughly evaluated for agreement with analytical solutions, expected behaviour, and consistency or improvement when compared to more established methods. Chapter \ref{chapter:NumericalValidation} tested the software and computational methods, when reproducing the well-balanced behaviour, moving wet-dry fronts, laboratory-scale dam-breaks, flash flooding from intense rainfall, and a real-life historical dam failure.

There are limitations to the accuracy of the first-order method in some cases, such as reproducing constantly changing wet-dry fronts in a parabolic bowl, where the result diverges over time because of numerical diffusion. However, the second-order method provided behaves as expected in this case, stressing the importance of selecting an appropriate numerical method for the flood mechanisms under consideration. Recommendations based on the results herein and the author's experience are provided in Table \ref{MethodRecommendations}. There are of course also limitations to the assumptions of the shallow water equations, particular with respect to dam-break simulations, but not sufficiently concerning to justify full three-dimensional modelling in the vast majority of cases.

\begin{table*}[tpb]
	\small
	\centering
	\caption{Recommendations for the suitability of the different numerical schemes in application to real-world cases}
	\label{MethodRecommendations}
	\begin{tabular}{p{0.2\linewidth}p{0.18\linewidth}p{0.53\linewidth}}
		\hline
		Scenario 											& Method				& Justification \\
		\hline
		Pluvial flooding									& First-order			& Short flow pathways are likely to see minimal effects of diffusion when compared to uncertainty in the topographic data, and limitations of the shallow water assumptions, especially in dense urban environments. Second-order method may prove beneficial in some hillslope scenarios with much longer flow pathways, where timing of confluence is important. \\
		\raggedright{Fluvial flooding from overtopping}		& First-order			& Low velocities do not justify the need for second-order methods except where extreme gradients exist. \\
		\raggedright{Fluvial flooding from defence failure}	& First-order			& High velocities similar to a dam break may be encountered, but these will often be short-lived, and the parametric uncertainty and limited topographic data are likely to have far more influence on the flood extent than numerical method (e.g. Thamesmead). \\
		\raggedright{Estuarine or tidal flooding}			& Second-order			& Second-order method may be required to improve representation of bore waves, and wave patterns of decelerating flow, travelling upstream over an emerging bed. \\
		Dam failure											& Second-order			& High velocities mean first-order method diffusion could have a marked impact on the arrival time of the flood, which is often of interest (e.g. evacuation scenarios). \\
		\hline
	\end{tabular}
\end{table*}

Pluvial flooding represents a different set of challenges, in which the volume from rainfall is gradually added, leading to shallow depths which are beyond some numerical schemes. The Environment Agency has a standard set of benchmarks against which commercial flood simulation software is compared, and their Glasgow hypothetical pluvial event is an ideal example. The results generated herein are most similar to those of TUFLOW FV, but all software solving the full shallow water equations created similar results, and a second-order method was not required. 

In addition to the analytical and hypothetical test cases, some events considered by other authors were simulated. The Carlisle flooding was reproduced with remarkable accuracy by using a $2m$ grid resolution, and therefore millions of cells. The performance metrics with respect to the post-event survey in this case exceed other published studies, however such was the nature of the event, that shock-capturing numerical schemes were not necessary for accurate results. Simulation of a hypothetical defence failure event in Thamesmead provided results consistent with published research, but when examining sensitivity to topographic data and grid resolution, an enormous variation was found. This raises important questions about the validity of flood risk analyses for defence and dam failure, characterised by the instantaneous release of large volumes, if models do not capture the complexity of the urban environment, and therefore underestimate velocities and extent. The author believes this is worthy of further research, to also consider the sensitivity in steep-sided settlements higher up in valleys.

Previous advocates of heterogeneous computing have suggested 32-bit floating-point arithmetic provides acceptable results, despite the truncation of data during each iteration. The results presented herein do not support this, and found major discrepancies in areas with complex flow conditions during the flooding following the Malpasset dam failure. Using 64-bit floating point is always recommended.

\section{Limitations of data sources available}

There remain events in which data is scarce, such as the lack of definitive validation data to professional survey standards for pluvial flood events. It will not be feasible to collect data for these in the same manner as traditional wrack and debris surveys, thus alternatives must be sought for post-event reconstructions. The large response to a photograph crowd-sourcing campaign following the Newcastle flood was invaluable in facilitating simulation, but these submissions were received in the days and weeks that followed.

Two different types of post-event survey were considered herein, including a water mark and wrack survey following the Carlisle 2005 floods. These surveys, while extremely helpful, focus on the maximum depth of a flood rather than the risk created; small depths with high velocities are extremely dangerous but often overlooked in data collection. Videos received from the public following the Newcastle flood were helpful in this respect, showing members of the public unable to stand, and rights of way carrying torrents. The questionnaire completed by residents affected in Newcastle demonstrates the complexities of flood impacts, where simulations struggled to predict accurately whether internal flooding would occur, but fared better for gardens and roads.

This thesis has made no attempt to consider the varying contributions of the drainage network and infiltration during flood events, but future work should. Sewer network data varies in accuracy and completeness, but considering this network would alleviate some of the depth overestimations in Chapter \ref{chapter:ScaleEffects}, and underestimation in cases of downslope surcharging. Sewer simulation and infiltration in real-time simulation systems will require additional sensing to provide the greatest benefit, such as depth sensors in pipes, and soil moisture measurements in large undeveloped areas, all of which are achievable in the near future.

Airborne altimetric LiDAR is an invaluable topographic data source, capturing much of the urban environment at low expense. Supplementing this data with ground-surveyed topographic datasets, such as OS MasterMap, further improves simulations. However, there are omissions in both these cases, such as reliable data for grade-separated infrastructure (i.e. underpasses, railway bridges and tunnels, culverted watercourses, and cantilevered structures) which can, as in Newcastle, act as a conduit for large volumes of water. A greater emphasis by mapping agencies on capturing all topographic data, as opposed to the highest features appearing on conventional maps, would further assist flood risk analyses.

Reports of flooding on social media were sufficiently detailed to approximate the rainfall accumulation for a flood in Newcastle on 28 June 2012, but reports were less forthcoming for a comparable event on 5 August 2012. We must exercise caution when using data sources with limited reach, which may not be representative of the population or city as a whole. Social media reports on Twitter largely focused on commuters, and hence the arterial connections to the city, with data scarce in some of the worst affected areas.

During an event, social media, CCTV, and the multitude of other pervasive sensors all provide viable real-time data, but this requires the efforts and coordination of interested parties, to facilitate timely data access.

\section{Recommendations for professional practice}

The author strongly believes that professional practice must move towards consuming new data sources, in as great a detail as possible, when conducting flood simulation. The results in Chapters \ref{chapter:Fluvial} and \ref{chapter:ScaleEffects} clearly demonstrate, that results are improved by doing so, up to a point of diminishing returns, at approximately $2m$ grid resolutions.

Whilst in hydrological simulation, there are a great many variables which are difficult to quantify, justifying the use of conceptual modelling, in hydraulics our understanding of the physical processes and depiction by data of the land surface, is comparatively comprehensive. Data costs are minimal following greater emphasis globally on open data initiatives, and computing power as a commodity is extremely cheap, with cloud resources and consumer-grade GPUs readily available.

Parametric sensitivity on the floodplain decreased with increasingly refined grid resolutions in the simulations of Carlisle, a gradual flood inundation, because flow was correctly constrained to the meandering river channel rather than circumventing it. As an engineering community we should strive to create models which reflect the actual physics we know to be correct for flood events, and increase transparency around our levels of uncertainty by communicating limitations of our models. The Thamesmead simulation demonstrates these limitations, where the velocities create sensitivity to parameterisation, and it is essential that the worst case is presented, albeit of low probability; failure to use all of the data available would not have captured this worst-case.

Flood simulation software products which cannot leverage multiple CPU cores or heterogeneous devices, should be retired from use in the near future; they cannot keep pace with technological developments, hence cannot leverage new data sources.

\section{Potential for real-time high-resolution flood forecasting and nowcasting}

Computing power and technology is ready to facilitate faster-than-realtime simulation of fluvial and pluvial events, and with further effort could simulate at the catchment scale. In fluvial events, statistical and one-dimensional modelling is already applied for flood forecasting in downstream areas, then related to precomputed flood events for a library of return-period based events. This approach is inherently conservative however, and leads to flood alerts and warnings issued which never transpire in reality. Real-time modelling provides an opportunity to create event-specific bespoke flood forecasts, and could easily be extended to consider uncertainty by employing an ensemble approach encompassing hydrological, meteorological, and parametric variations. The author is keen to explore this further, and create a demonstration.

In all likelihood, the flood model employed in a real-time system is likely to provide a fraction of the uncertainty when compared to the hydrology and meteorology, which is far more difficult to quantify and forecast. When considering pluvial events, this is even more applicable. Convective storms pose the greatest risk for dramatic flooding from intense rainfall, within short timeframes, and yet these processes occur on a small scale and are difficult to predict. Nowcasting is a more viable option for pluvial flooding, in which national networks of rainfall radar, rain gauges, and river level gauges provide sufficient information to reproduce the situation on the ground, potentially further improved by leveraging data sources such as image-processed CCTV and social media. These results could prove valuable in delivering an effective emergency response with limited resources.

There is a disconnect between the forecasting methods for different flood mechanisms in the UK, which remains unchanged despite its acknowledgement in \citet{Pitt2007}, compounded by the different authorities responsible for each. A comprehensive unified real-time flood simulation system capturing the multitude of data available would be achievable, and could even consider the combinatorial effects of fluvial, pluvial, groundwater, and tidal mechanisms. It is hoped the regulatory responsibilities are changed in the near future, and such a system considered.
