\chapter{Literature review}
\label{chapter:Background}

This chapter will consider the history of flood simulation, with particular reference to the UK, and further elucidate the rationale behind this thesis. In particular, as a wide range of techniques and methods are employed for flood simulation, the merits of these, constraints, and potential for technological advances to alleviate those, is considered.

Further literature is reviewed within the specific context of numerical methods for the shallow water equations in Chapter \ref{chapter:NumericalMethods}, and for domain decomposition in Chapter \ref{chapter:Decomposition}.

\section{Flood simulation}

An essential component of engineering design and risk analysis, is to plan for events with magnitudes exceeding those previously observed. Statistical analysis is used with respect to historical climate data to derive intensity-duration-frequency (IDF) curves for rainfall events. The 1\% annual exceedance probability (AEP) is often used as a benchmark against which infrastructure should be defended in the United Kingdom, which is equivalent to (but by no means a limiting factor) a 1-in-100 year probability of occurrence. Establishment of these IDF estimates is the first step in the process outlined by the UK's industry standard Flood Estimation Handbook \citep{Faulkner1999}. Infrastructure which is critical or politically sensitive may be designed with defence to higher standards, such as nuclear power stations and parts of Central London. These statistical approaches only provide rainfall events; modelling is necessary to establish the effects of said events upon river levels, drainage networks, estuarine zones, and surface water, and the consequences of these changes, which may be economic and social.

A multitude of modelling approaches have been applied to relate a rainfall event to its hydrological outcomes, ranging from simple statistical approaches, to physically-based methods. Notable examples include transfer function models and extensions thereof \citep[e.g.][]{Moore2007}, physically-based distributed hydrological models representing the catchment in three-dimensions but with simplified representations of in-channel physics owing to data and computational constraints \citep[e.g.][]{Abbott1986}, neural network-based learning methods \citep[e.g.][]{Dawson2001}, and those focusing on event-based simulation, with varying levels of simplification to the physical processes within channels \citep[e.g.][]{Hunter2007}.

A consistent theme in the aforementioned works is the practicalities of achieving sufficiently accurate results within the constraints of computational power. \citet{Hunter2007} reports that three-dimensional modelling of flow dynamics is "not viable at the reach-scale (1km)," a situation which has changed little to-date.

Physical processes which are not explicitly considered by the numerical methods are often adjusted for through calibration of model parameters, such as the Manning coefficient, originally conceived to account for energy losses within a one-dimensional representation of a river reach, but frequently referred to as a friction coefficient \citep[e.g.][]{Hunter2005,Gallegos2009,Fewtrell2008a}. Calibration of these models is dependent upon a range of observations and statistical methods to evaluate the model performance; depending on the type of model employed, there is no guarantee the calibrated model will be transferable following changes in channel geometry, bed conditions, or floodplain topography, hence reduction in sensitivity is desirable.

Applications of these models fall into two broad categories: event-based short-term, and long-term water balance simulation. The latter is often of interest for water resource management, climatology, and to an extent meteorology \citep[e.g.][]{Gudmundsson2012}. The former is applied retrospectively in flood event analyses, for short model runs in a predictive capacity, and in scenario-based flood risk analyses, which are often based upon a hypothetical short duration derived from an exceedance probability. Physical simulation of the behaviour in water remains an extremely complex undertaking, however for the purposes of many engineering applications, many deemed that considering flow variance with depth is unnecessary \citep{Toro2001}. This depends upon the flow under consideration, with dam-breaks arguably the least appropriate for reduced-dimension modelling \citep{Liang2010}.

Considering current practice in event-based modelling, and the limitations thereof, the UK's surface water flood risk maps are derived from software based upon the shallow water equations, initially with no hydrological (e.g. saturation) considerations \citep{EnvironmentAgency2013}. Fluvial flood forecasts in England are based upon a mixture of conceptual and physically-based modelling approaches, because of constraints in data availability and the maximum run-time to be operationally useful. The National Flood Risk Assessment (NaFRA) is also derived from scenario-based modelling, with the techniques applied often improved with each iteration, although nonetheless the subject of much criticism \citep{PenningRowsell2014}. Historic details of the NaFRA methodology are no longer published on the web, however the methodology does not engage high-resolution physically-based models, owing to the data demands and computational requirements for doing so (Dawson, personal conversation).

This thesis focuses upon event-based simulation, for which it is feasible to use physically-based methods that solve the shallow water equations, for applications beyond the reach scale. These equations are obtained by depth-integration of the Navier-Stokes equation.

\section{Physically-based modelling of shallow flows}

The shallow water equations may be solved using a variety of methods. Finite-difference methods, where the derivative terms are substituted with a finite-difference approximation, remain commonplace and are employed by many of the commercial software \citep{Dyke2007,Hunter2008, BMTWBM2011a}. Unfortunately, whilst simple to implement, these methods are less reliable when discontinuities are encountered, in situations arising around high velocities, steep slopes, and hydraulic jumps \citep{Preiswerk1940}. For such situations, a total variation diminishing (TVD) scheme may be adopted, which will remain stable, and whilst not necessarily capable of representing the exact solution, will not produce unphysical results \citep{Harten1997,Laney1998}. Considering the integral form of the equations allows mass and momentum to be conserved, thereby allowing shocks to be captured, through classes of methods described as either finite-volume or finite-element, solved on the basis of fluxes across cell boundaries. A complication however, is these methods are constrained if they are to remain TVD; meeting these constraints can increase the amount of computation involved, insofar as limited timestep or additional calculations \citep{Courant1967}. Finite-volume methods are shown to provide more accurate results than finite-difference, for the complex situation of dam-break simulations \citep{Zoppou2003}. Similarly, they can also achieve these results with less computational expense than finite-element methods \citep[e.g.][]{Lukacova-Medvidova2006}.

By adopting an appropriate method, these finite-volume shallow flow models can be applied across a wide range of scenarios to simulate pluvial and fluvial events, and the consequences of dam collapse. Amongst the most advanced models are Godunov-type schemes, allowing both accurate and stable simulation even where complex flow dynamics exist (e.g. hydraulic jumps), and thereby facilitating the use of high-resolution datasets, now available at low expense with LiDAR data. Generally speaking, the higher the resolution of the data, the more likely steep slopes and complex terrain will be captured, and thus the likelihood of complex flow scenarios is increased. Auspicious engineering design and risk analysis increasingly demands these high levels of detail and accuracy \citep{Marks2000,French2003,Haile2005}. To capture highly transient complex hydrodynamic processes, i.e. those induced by dam breaks, a Godunov-type scheme is normally adopted, using an explicit scheme with time integration, which imposes a strict constraint on the timestep.

The considerable computational expense associated with solving the shallow water equations in two dimensions, has led to research directly focused on expedient computation of results, often at the expense of accuracy. A considerable body of literature focusses on simplification of the underlying equations to create kinematic- or diffusive-wave approximations. However, most of these simplified models are not appropriate to depict complex catchment responses arising in steep headwaters, or dense urban environments, whilst providing accurate depths and velocities. Their reduced physical complexity may cause increased sensitivity to and dependence on parameterisation \citep{Costabile2009,Costabile2012,Fewtrell2011a,Yeh2011}. Furthermore, there is evidence to suggest these simplified approaches will not achieve significant and consistent reductions in computation time, albeit case and resolution dependent \citep{Hunter2007,Pender2010,Wang2011a}.

To improve the computational efficiency of the diffusion-wave models for high-resolution simulations, \citet{Bates2010} presented a new formula for estimating inter-cell discharges for this type of models by partially restoring the inertial terms from the fully dynamic momentum equation. The numerical stability of the resulting partial inertial model was thus controlled by the much less restricted Courant\textendash Freidrichs\textendash Lewy (CFL) condition, in the same form as that applied to the explicit hydrodynamic models. Due to the use of simplified governing equations and a reduced complexity numerical scheme, the partial inertial model was shown to save 20\% - 30\% of the computational cost, when compared with a shock-capturing hydrodynamic model that solves the full two-dimensional shallow water equations in a similar code base \citep{Zhang2014}. However, for a city-scale high-resolution flood simulation, that covers millions of computational nodes, the partial inertial models are still computationally too demanding. Furthermore, \citet{Neal2012}, after comparing the performance of their diffusion-wave, partial inertial and shock-capturing dynamic wave models against a set of benchmark test cases, pointed out that while the simplified models may reproduce numerical results comparable to a full set of equations, they are unable to simulate supercritical flows accurately. Crucially, urban flash flood events as a result of intense rainfall or failure of flood defences are generally characterised as rapidly varying trans-critical and supercritical flows. Therefore, shock-capturing dynamic models, representing recent developments in the simulation of complex shallow flow hydrodynamics, appear to offer an appropriate compromise between capability, expediency, and accuracy, within the domain applications of urban flash flood modelling. It is acknowledged, that these approaches are nonetheless incapable of representing certain small-scale three-dimensional flow features, and this can cause inaccuracies \citep{Soares-Frazao2008,Guinot2012}. 

There are obvious limits to applying a high-resolution model across an entire region or country, to directly capture small-scale topographic or flow features. Different sub-grid parameterisation techniques have also been proposed to integrate high-resolution topographic features into flood models, to enable more accurate and efficient coarse-resolution simulations \citep[e.g.][]{Soares-Frazao2008,Guinot2012,Schubert2012,Chen2012}. Most of these models are essentially based on reformulation of full dynamic shallow water equations, to effectively reproduce complex urban topography. \citet{Soares-Frazao2008} introduced a new shallow flow model with porosity to account for the reduction in storage and conveyance due to sub-grid topographic features. The performance of the porosity model was compared with that of a refined mesh model explicitly reflecting sub-grid scale urban structures, and an often-employed approach of raising local bed roughness. While able to reproduce the mean characteristics of the urban flood waves at a much lower computational cost than the refined mesh simulations, the porosity model was unable to accurately predict the formation and propagation of certain localised wave features, e.g. reflected bores. In another study, \citet{Schubert2012} investigated various approaches of representing sub-grid topographic features in simulating a dam-break flood in an urban area and concluded that only those methods taking account of building geometries can capture building-scale variability in the velocity field. They also indicated the benefit of using high-resolution simulation to explicitly represent buildings and road structures, if run-time execution costs were not a major concern \citep[see also][]{Gallegos2009}.

These numerical methods are susceptible to numerical diffusion, hence there are limitations to the scope and application of first-order methods in solving the shallow water equations. These limitations are especially apparent when considering dam-break style scenarios, characterised by high velocities, supercritical flow conditions, and a necessity for accurate velocity predictions in applications of risk analysis and warning systems. A variety of methods exist for achieving higher-order solutions, and there exists a trade-off between the numerical accuracy of the solution, and the magnitude of error which cannot be mitigated, introduced by uncertainties in the environment and source data (e.g. inaccuracies in LiDAR data, or a failure to capture topographic complexity within the spatial discretisation).

Literature continues to consider different methods for achieving higher order solutions. \citet{Kesserwani2014} compared two methods, the Monotonic Upstream-Centred Scheme for Conservation Laws (MUSCL) approach, and Runge-Kutta Discontinuous Galerkin (RKDG), within the context of shallow flow modelling for flood simulations; they conclude that the increased complexity and accuracy of the RKDG method provides benefits with coarse resolution meshes, primarily for data-scarce applications. The focus herein is high-resolution modelling with expediency, hence MUSCL appears a robust choice.

\section{HPC- and GPU-based flow modelling}

If software developers are to keep pace with the increasing power of hardware, they must accept and respond to the stagnation of central processing unit (CPU) clock speeds, and look to parallel processing to fully harness the available computing power. Despite this, the majority of commercial hydraulic modelling software can only utilise a single CPU core. Conversely, parallel programming approaches are shown to exhibit good scaling when software is structured appropriately \citep[e.g.][]{Neal2010,Saetra2012}.

There is a far more interesting potential for looking beyond the CPU, and examining the role heterogeneous computing might play, in which more than one processor architecture is available for use within a single computer system, as is increasingly common. Graphics Processing Units (GPUs) are designed to process large volumes of data by performing the same calculation numerous times, typically on vectors and matrices. Such hardware architectures are well-suited to the field of computational fluid dynamics, which has mathematical similarities. New programming languages including CUDA and OpenCL have exposed this hardware for use in general-purpose applications, referred to as GPGPU \citep{Owens2007,Nickolls2010}, without the need to author code in languages intended and designed for graphical operations, although early pioneers of heterogeneous computing did just that.

A number of attempts have been made to explore the benefits of GPU computing for highly efficient large-scale flood simulations. Early pioneers of such methods include \citet{Crossley2009} who harnessed graphics APIs directly to implement a diffusion wave model (JFlow) for GPUs, \citet{Kalyanapu2011} with a finite-difference implementation of the full shallow water equations, and later \citet{Brodtkorb2012} with a finite-volume scheme. Such software is becoming increasingly mainstream; \citet{Pender2013} report results from several commercial GPU hydraulics implementations while \citet{Smith2013} demonstrate the potential for generalised approaches applicable to both CPU and GPU co-processors. The most recent research also explores how domain decomposition across multiple GPUs can provide further performance benefits \citep{Saetra2012}.

Successful GPU-computed models have already been applied in computational fluid dynamics (CFD) for astrophysics, magneto-hydrodynamics, haemodynamics and gas dynamics \citep[e.g.][]{Bisson2012}. A GPU designed for scientific use can be expected to boost performance by approximately 6.7 times compared to a typical quad-core CPU device with 64-bit floating-point computation, and assuming architecture-efficient implementations \citep{IntelCorporation2012,NVIDIACorporation2011}. Importantly however, performance benefits can scale with multiple devices \citep{Kuo2011,Saetra2012}. Advantages are evident across a wide range of different approaches, numerical schemes and spatial discretisations \citep[e.g.][]{Kuo2011,Horvath2010,Wang2010,Rossinelli2011,Schive2011,Crespo2011}. Suitability and successful application for the shallow-water equations is evidenced with both a Kurganov-Petrova scheme and split HLL method \citep{Brodtkorb2010,Brodtkorb2010a,Brodtkorb2011,Saetra2012,Kuo2011}, however current literature focusses on Riemann solver-free schemes, 32-bit floating-point, and vendor-specific implementations (primarily CUDA). Computationally intensive schemes of higher orders in time using more generalised Riemann solvers which consider contact and shear waves, are worthy of further research.

Another technique for expediting shallow-water simulations is domain decomposition, in which different physical machines or processing devices are responsible for smaller elements of a single domain. There is a long record of these techniques being employed, such as \citet{Rao2004}, \citet{Neal2010} and \citet{Asuncion2016}, however the magnitude of performance improvements tends to degrade as more devices are added. Many of the software to-date has focused on single applications \citep[e.g.][for tsunami]{Asuncion2016}, whereas the intention herein is to develop software as versatile as possible, and demonstrate its application across a range of flood mechanisms.

\section{High-resolution modelling in practice}

The constraints imposed by computational performance have prohibited the highest resolution data from broad-scale applications, with notable exceptions (e.g. the aforementioned JFLOW-GPU used for the national surface water flood risk assessment). Consequently, there is a limited body of research describing the advantages or limitations of applying flood models at these high resolutions, especially $<$5m.

The premise of a resolution which constitutes `high-resolution' modelling has also changed during the last twenty years, such as the 25m cell sizes used by \citet{Bates2003} versus 0.1m by \citet{Ozdemir2013}. A small number of grid sensitivity studies were conducted when high-resolution LiDAR became prevalent \citep[e.g.][]{Haile2005}, but these did not extend to consider sensitivity to parameterisation. \citet{Ozdemir2013} implemented simulations at resolutions as detailed as 0.1m, using data obtained from terrestrial LiDAR surveys, and demonstrate substantial benefits from these extremely high levels of detail. However, they also report the run-times of these simulations as prohibitively lengthy for broader-scale application. A prior study by \citet{Fewtrell2011} focused on the same area, with similar conclusions, but simplifications of the shallow water equations were used, and in both cases a domain was selected that was unlikely to give rise to shocks or discontinuities in the flow, limiting the scope of conclusions.

The \citet{EnvironmentAgency2013} describes the results of sensitivity testing conducted by JBA Consulting as part of their surface-water flood risk mapping, but unfortunately information on the methodology employed is not provided. Most recently, variable grid resolutions have been considered for catchment-scale modelling, with high resolution grids focusing upon the population centres \citep[e.g.][]{Hartnett2017}; this is an area likely to see further work in future years, potentially with different numerical schemes employed, as in \citet{Zhang2015}. \citet{Hartnett2017} reported substantial benefits from leveraging high-resolution LiDAR data, but stressed the importance of work to clean and prepare the data, particularly in forested areas.

\section{Conclusion}

Flood modelling takes many forms and has been employed throughout the last fifty years to inform strategic decisions and engineering practice. Throughout this period, the ability for computing technology to deliver results within acceptable timeframes has consistently been a limiting factor, and will remain to be until a major advance in technology. Nonetheless, a substantial body of research exists exploring methods for expediting simulations, how much complexity is necessary, and methods for removing complexity whilst minimising the effect on results.

During this time, the availability of data has vastly increased, with approximately 70\% of England and Wales now surveyed for openly-available airborne altimetric LiDAR data (\url{http://environment.data.gov.uk/ds/survey/}), and terrestrial LiDAR data now routinely collected for future autonomous vehicle use. There has been far less research focusing on how to consume and utilise all the data available, even if this requires substantial computing power. Work undertaken since \citet{Brodtkorb2010} has made it abundantly clear that as much effort should focus on how best to capitalise upon, and leverage the processing power available, as was previously dedicated to simplification for expediency.

Within this thesis, where the computational methods are later shown to reduce simulation times sufficiently to facilitate broader use of high-resolution datasets, it is necessary to build upon the literature by fully considering the sensitivity to grid resolutions and parameterisations which result from detailed high-resolution simulation.
