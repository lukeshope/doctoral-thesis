\chapter{Introduction}
\label{chapter:Introduction}

\section{Background}

The United Kingdom has suffered through numerous storms and severe floods throughout the last decade, most notably in 2007 and 2012, the wettest year for a century. These events resulted in lives lost and millions of pounds worth of damage. The UK was not alone, with similarly unusual weather events reported across the globe. As an example, in June 2013 an extreme flood event affected a substantial area of central Europe, resulting in 25 deaths and more than \euro{12bn} losses. This is not a new problem; the UK Environment Agency attributed two thirds of flooding during 2007, when a wet season caused widespread disruption, to surface water and inadequacies in drainage \citep{Pitt2007}. In England alone over 5.2 million properties are known to be at risk of flooding, and annual investment must increase to more than a billion pounds per year just to maintain current levels of protection under the threat of climate change \citep{EnvironmentAgency2009a}.

Decisions regarding future investment in defences, and operational priorities during major incidents are made with the assistance of software modelling. Retrospectively, data is collected on the flood extent and losses, which then has applications in determining the best location for defences, drainage upgrades, and ‘soft engineering’ strategies (i.e. warning systems, sandbags, insurance, planning constraints). When combined with meteorological and hydrological data, and statistical methods such as those found in the Flood Estimation Handbook \citep{Faulkner1999}, we can make informed decisions with respect to our accepted levels of risk. Hydrodynamic modelling serves to bridge the gap between hydrology and climatology, and the impacts on property, infrastructure, and people. The ever-increasing availability of data is also a factor to consider, whereby 72\% of England is covered by highly-detailed airborne altimetric LiDAR data, and social media and citizen science provide a real-time stream of data while events unfold.

Hydrodynamic models in the form of free-surface shallow-flow modelling, can be applied across a wide range of scenarios to simulate flooding resulting from fluvial, pluvial, and tidal sources, tsunami, or dam failure. Amongst the most advanced models are Godunov-type schemes, allowing accurate simulation even where complex flow dynamics exist (e.g. hydraulic jumps), and utilising high-resolution datasets. These are now readily available at minimal expense with airborne altimetric LiDAR capable of capturing swathes of data in a single flight, and an increasing movement towards licence-free open data. Auspicious engineering design and risk analysis increasingly demands these high levels of detail and accuracy \citep{French2003,Haile2005,Marks2000}. To capture highly transient complex hydrodynamic processes, e.g. those induced by dam breaks, a Godunov-type scheme is normally developed using an explicit scheme in time integration, which imposes a strict constraint on the timestep. However, these explicit time-marching schemes are computationally expensive, thus high-resolution simulations across large catchment or city scales are often unfeasible in practice without super-computers, despite recent substantial developments in CPU power.

To overcome computational constraints, numerous types of acceleration have been explored previously. Dynamic grid adaptation delivers limited benefits (2-3x faster) for complex flow characteristics, but introduces challenges for managing mass and momentum conservation during refinement \citep{Liang2004}. Many high-performance and high-throughput computing (HPC/HTC) techniques, such as distributed computing (e.g. Condor) are constrained by communication because of the interdependency of the solution between cells and their neighbours, making scalable implementations difficult to accomplish \citep{Pau2006,Delis2009}, although not impossible.

Elimination of terms from the shallow water equations (SWEs) gives rise to kinematic and diffusive wave approximations. These may allow faster computation at the expense of losing physical complexity, but compromise the predictive capabilities for flow velocity and by extension the time of inundation. Diffusive approximations are hindered at high resolutions by a strict timestep constraint, and kinematic approximations neglect potentially significant aspects of flow such as backwater effects \citep{Bates2000,Tsai2003,Hunter2005}. The topographic features of urban environments are characterised by narrow gaps between buildings and steep gradients; such conditions all have the potential to create transient flows, increase velocities, induce shocks, and cause backwater effects \citep{Testa2007,ElKadiAbderrezzak2011,Xia2011}. Comprehensive analysis of simplified approaches and the errors therein from numerical and pragmatic perspectives is given by \citet{Singh1996}, \citet{Hunter2007}, and \citet{Pender2010,Pender2013}. Simplified models are known to produce good results when calibrated against observed inundation data \citep[e.g.][]{Neal2009,Horritt2010} but parametric uncertainty and sensitivity is problematic for simulating flood scenarios for an uncertain future \citep{Horritt2002,Yu2006,Fewtrell2008a}.

Whilst transistor counts on central processing units (CPUs) continue to rise at rates comparable to Gordon Moore's observations, clock speed increases have stalled. Software developers must increasingly look to multi-core processing if they are to fully leverage the power of modern computers. Despite this, a large number of commercial hydraulic modelling packages still provide no such functionality, even though good weak and strong scaling can be accomplished \citep{Sanders2010,Kalyanapu2011,Saetra2012}, defined as the variation in solution time for a fixed problem size per processor, and fixed total size, respectively. Heterogeneous computing, in which processors of different specialised architectures coexist in a single machine, and the advent of new methods (i.e. CUDA, OpenCL, DirectCompute) of interfacing with graphics processing units (GPUs) are even more promising: these devices are well-suited to performing the same calculation across large datasets, and are ideal for computational fluid dynamics (CFD). Much of the literature focuses on CUDA implementations which are constrained to operating on NVIDIA hardware \citep[e.g.][]{Kuo2011,Saetra2012}. Commercial software options for hydraulic modelling on GPU architectures are now available; a comparison can be found in \citet{Pender2013}, albeit limited to small domains which cannot fully capitalise on the parallel processing benefits \citep{Saetra2012}. The majority have yet to be released to the public, and all those the authors are aware of utilise CUDA and are hence limited to operating on NVIDIA hardware.

Commercially viable software must also be resilient to hardware differences in capacities and architectures. The numerical methods employed must be applicable across a wide range of scenarios, and able to preserve depth positivity, capture shocks (flow discontinuities), appropriately manage wet-dry interfaces, and handle complex domain topography or exhibit the so-called well-balanced property \citep{Xing2010,Murillo2010}. Few generalised modelling tools with all these qualities currently exist, capable of leveraging the latest developments in computing hardware. 

Herein, computational methods are developed to satisfy these requirements, and applied, to provide a next-generation shallow-flow modelling tool that can readily be applied for different purposes (i.e. different flood simulations). By increasing the physical complexity and discretisation used in flood simulation, new questions are generated, such as how much complexity is necessary to reproduce behaviour encountered in the real-world. These are explored with analysis of a range of model complexities, grid resolutions, and parameterisations, to demonstrate capacity for detailed whole-city simulation, and real-time systems.

\section{Aim and objectives}

This thesis aims to develop and demonstrate computational methods to advance the potential accuracy and utility derived from flood simulation, by leveraging the latest computational developments and increased availability of data, such as LiDAR and social media.

This will be accomplished through the objectives:
\begin{enumerate}
	\item develop computational methods and techniques, and implement these, to leverage the most recent advances in computing power, for the purposes of expedited flood simulations, and assess the magnitude of performance benefits these techniques may provide;
	\item comprehensively demonstrate the software's ability to provide results comparable or improving upon existing software, for a wide range of flow conditions;
	\item evaluate the limitations of different data sources available for flood simulation through application and analysis, including data which is topographic, meteorological, hydrographic, post-event surveys, and social media;
	\item consider whether there is a point of diminishing returns, with regard to numerical and spatiotemporal accuracy in flood simulation, and thereby make recommendations for professional practice; and
	\item determine the feasibility and obstacles to delivering real-time high-resolution flood forecasting or nowcasting, with respect to the different flood mechanisms.
\end{enumerate}

\section{Thesis outline}

The remainder of this thesis is structured as follows.

\begin{itemize}
	\item \textbf{Chapter \ref{chapter:Background}} provides an initial review of the current state of practice in flood simulation, and the tools and data employed at present. Further background is introduced where appropriate in subsequent chapters, such as the numerical methods in Chapter \ref{chapter:NumericalMethods}, and the real-world events considered herein.
	\item \textbf{Chapter \ref{chapter:NumericalMethods}} describes the Godunov-type methods in full detail, and provides discussion on the methods for achieving higher-order solutions, and solving the Riemann problem. The chapter then goes on, to describe how these methods may be implemented to leverage the latest technological developments, and discusses methods for achieving the highest computational throughput.
	\item \textbf{Chapter \ref{chapter:NumericalValidation}} validates the numerical methods, as implemented with the new computational techniques, against a wide range of analytical, laboratory-scale, and hypothetical real-world test cases, with comparison against analytically-derived results and comparable research and software outputs.
	\item \textbf{Chapter \ref{chapter:Fluvial}} applies the methods to a full-scale real-world major flood event in Carlisle during 2005, and critically analyses results obtained with different spatial resolutions and parameterisations, then considers a hypothetical breach in the defences at Thamesmead to further test hypotheses with a high-velocity event.
	\item \textbf{Chapter \ref{chapter:Decomposition}} examines how the numerical methods may be safely decomposed to spatially disaggregate computation across multiple nodes, and provides results using these methods to demonstrate the numerical result is not compromised.
	\item \textbf{Chapter \ref{chapter:ScaleEffects}} considers how accurate the solution is required to be, and how much benefit can be derived from increasingly high resolutions of input data, by comparison of simulation results against real-world data from a variety of sources for a major pluvial flood event in Newcastle upon Tyne.
	\item \textbf{Chapter \ref{chapter:SoftData}} provides a reverse simulation using data sourced solely from social media, to correlate reputed flood consequences against simulations and determine rainfall intensity and duration.
	\item \textbf{Chapter \ref{chapter:Conclusions}} provides conclusions and recommendations.
\end{itemize}
